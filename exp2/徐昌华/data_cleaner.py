#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import re
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

class UniversalDataCleaner:
    def __init__(self, input_file: str, output_file: str):
        self.input_file = input_file
        self.output_file = output_file
        self.df = None
        self.original_df = None
        self.cleaning_log = []
        
    def log_action(self, action: str):
        """è®°å½•æ¸…æ´—æ“ä½œ"""
        print(f" {action}")
        self.cleaning_log.append(action)
        
    def load_data(self):
        """æ™ºèƒ½åŠ è½½CSVæ•°æ®ï¼Œè‡ªåŠ¨æ£€æµ‹ç¼–ç """
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'cp1252', 'utf-16']
        
        for encoding in encodings:
            try:
                self.df = pd.read_csv(self.input_file, encoding=encoding)
                self.original_df = self.df.copy()
                print(f" æˆåŠŸåŠ è½½æ•°æ®ï¼ˆç¼–ç ï¼š{encoding}ï¼‰")
                print(f"  - æ•°æ®ç»´åº¦ï¼š{self.df.shape[0]}è¡Œ Ã— {self.df.shape[1]}åˆ—")
                print(f"  - åˆ—åï¼š{list(self.df.columns)}")
                return True
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f"  å°è¯•ç¼–ç  {encoding} å¤±è´¥ï¼š{e}")
                continue
        
        print(" æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥ï¼")
        return False
        
    def auto_detect_column_types(self):
        """è‡ªåŠ¨æ£€æµ‹åˆ—çš„æ•°æ®ç±»å‹"""
        print("\n=== è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹ ===")
        
        column_types = {}
        
        for col in self.df.columns:
            # è·å–éç©ºå€¼
            non_null_values = self.df[col].dropna()
            
            if len(non_null_values) == 0:
                column_types[col] = 'empty'
                continue
                
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼å‹
            numeric_values = pd.to_numeric(non_null_values, errors='coerce')
            numeric_ratio = numeric_values.notna().sum() / len(non_null_values)
            
            if numeric_ratio > 0.8:  # 80%ä»¥ä¸Šæ˜¯æ•°å€¼
                if numeric_values.equals(numeric_values.astype(int, errors='ignore')):
                    column_types[col] = 'integer'
                else:
                    column_types[col] = 'float'
            else:
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¸ƒå°”å‹
                bool_values = non_null_values.astype(str).str.upper()
                bool_keywords = {'TRUE', 'FALSE', 'YES', 'NO', '1', '0', 'T', 'F', 'Y', 'N'}
                bool_ratio = bool_values.isin(bool_keywords).sum() / len(non_null_values)
                
                if bool_ratio > 0.8:
                    column_types[col] = 'boolean'
                else:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†ç±»å‹ï¼ˆé‡å¤å€¼è¾ƒå¤šï¼‰
                    unique_ratio = len(non_null_values.unique()) / len(non_null_values)
                    if unique_ratio < 0.1:  # å”¯ä¸€å€¼æ¯”ä¾‹å°äº10%
                        column_types[col] = 'category'
                    else:
                        column_types[col] = 'text'
            
            print(f"  {col}: {column_types[col]}")
            
        return column_types
        
    def detect_and_fix_missing_values(self):
        """æ£€æµ‹å’Œå¤„ç†ç¼ºå¤±å€¼"""
        print("\n=== å¤„ç†ç¼ºå¤±å€¼ ===")
        
        missing_info = self.df.isnull().sum()
        total_missing = missing_info.sum()
        
        if total_missing == 0:
            print(" æœªå‘ç°ç¼ºå¤±å€¼")
            return
            
        print(f"å‘ç° {total_missing} ä¸ªç¼ºå¤±å€¼ï¼š")
        
        for col, count in missing_info[missing_info > 0].items():
            missing_ratio = count / len(self.df)
            print(f"  {col}: {count}ä¸ª ({missing_ratio:.2%})")
            
            if missing_ratio > 0.5:  # è¶…è¿‡50%ç¼ºå¤±ï¼Œè€ƒè™‘åˆ é™¤åˆ—
                print(f"    è­¦å‘Šï¼š{col}åˆ—ç¼ºå¤±ç‡è¿‡é«˜ï¼Œå»ºè®®åˆ é™¤")
                # å¯ä»¥é€‰æ‹©åˆ é™¤è¯¥åˆ—
                # self.df = self.df.drop(columns=[col])
                # self.log_action(f"åˆ é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„åˆ—ï¼š{col}")
            else:
                # æ ¹æ®æ•°æ®ç±»å‹å¡«å……ç¼ºå¤±å€¼
                if self.df[col].dtype in ['int64', 'float64']:
                    # æ•°å€¼å‹ï¼šç”¨ä¸­ä½æ•°å¡«å……
                    median_val = self.df[col].median()
                    self.df[col].fillna(median_val, inplace=True)
                    self.log_action(f"ç”¨ä¸­ä½æ•°({median_val})å¡«å……{col}åˆ—çš„ç¼ºå¤±å€¼")
                else:
                    # æ–‡æœ¬å‹ï¼šç”¨ä¼—æ•°å¡«å……
                    mode_val = self.df[col].mode()
                    if len(mode_val) > 0:
                        self.df[col].fillna(mode_val[0], inplace=True)
                        self.log_action(f"ç”¨ä¼—æ•°({mode_val[0]})å¡«å……{col}åˆ—çš„ç¼ºå¤±å€¼")
        
    def detect_and_remove_duplicates(self):
        """æ£€æµ‹å’Œåˆ é™¤é‡å¤è¡Œ"""
        print("\n=== å¤„ç†é‡å¤æ•°æ® ===")
        
        initial_rows = len(self.df)
        duplicates = self.df.duplicated()
        duplicate_count = duplicates.sum()
        
        if duplicate_count > 0:
            print(f"å‘ç° {duplicate_count} ä¸ªé‡å¤è¡Œ")
            
            # æ˜¾ç¤ºé‡å¤è¡Œçš„ç¤ºä¾‹
            duplicate_rows = self.df[self.df.duplicated(keep=False)]
            print("é‡å¤è¡Œç¤ºä¾‹ï¼ˆå‰5è¡Œï¼‰ï¼š")
            print(duplicate_rows.head())
            
            # åˆ é™¤é‡å¤è¡Œ
            self.df = self.df.drop_duplicates()
            removed_rows = initial_rows - len(self.df)
            self.log_action(f"åˆ é™¤äº†{removed_rows}ä¸ªé‡å¤è¡Œ")
        else:
            print(" æœªå‘ç°é‡å¤è¡Œ")
            
    def detect_and_fix_outliers(self):
        """æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼"""
        print("\n=== å¤„ç†å¼‚å¸¸å€¼ ===")
        
        numeric_columns = self.df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_columns) == 0:
            print(" æ— æ•°å€¼åˆ—ï¼Œè·³è¿‡å¼‚å¸¸å€¼æ£€æµ‹")
            return
            
        outlier_info = {}
        
        for col in numeric_columns:
            # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]
            
            if len(outliers) > 0:
                outlier_ratio = len(outliers) / len(self.df)
                outlier_info[col] = {
                    'count': len(outliers),
                    'ratio': outlier_ratio,
                    'bounds': (lower_bound, upper_bound)
                }
                
                print(f"  {col}: {len(outliers)}ä¸ªå¼‚å¸¸å€¼ ({outlier_ratio:.2%})")
                print(f"    æ­£å¸¸èŒƒå›´ï¼š[{lower_bound:.2f}, {upper_bound:.2f}]")
                
                # å¦‚æœå¼‚å¸¸å€¼æ¯”ä¾‹ä¸é«˜ï¼Œå¯ä»¥è€ƒè™‘ç”¨è¾¹ç•Œå€¼æ›¿æ¢
                if outlier_ratio < 0.05:  # å°äº5%
                    self.df.loc[self.df[col] < lower_bound, col] = lower_bound
                    self.df.loc[self.df[col] > upper_bound, col] = upper_bound
                    self.log_action(f"å°†{col}åˆ—çš„å¼‚å¸¸å€¼é™åˆ¶åœ¨æ­£å¸¸èŒƒå›´å†…")
        
        if not outlier_info:
            print(" æœªå‘ç°æ˜¾è‘—å¼‚å¸¸å€¼")
            
    def standardize_data_types(self):
        """æ ‡å‡†åŒ–æ•°æ®ç±»å‹"""
        print("\n=== æ ‡å‡†åŒ–æ•°æ®ç±»å‹ ===")
        
        column_types = self.auto_detect_column_types()
        
        for col, dtype in column_types.items():
            try:
                if dtype == 'integer':
                    self.df[col] = pd.to_numeric(self.df[col], errors='coerce').astype('Int64')
                    self.log_action(f"å°†{col}åˆ—è½¬æ¢ä¸ºæ•´æ•°ç±»å‹")
                    
                elif dtype == 'float':
                    self.df[col] = pd.to_numeric(self.df[col], errors='coerce')
                    self.log_action(f"å°†{col}åˆ—è½¬æ¢ä¸ºæµ®ç‚¹æ•°ç±»å‹")
                    
                elif dtype == 'boolean':
                    # æ ‡å‡†åŒ–å¸ƒå°”å€¼
                    bool_map = {
                        'TRUE': True, 'FALSE': False,
                        'YES': True, 'NO': False,
                        'Y': True, 'N': False,
                        'T': True, 'F': False,
                        '1': True, '0': False,
                        'true': True, 'false': False,
                        'yes': True, 'no': False
                    }
                    self.df[col] = self.df[col].astype(str).map(bool_map)
                    self.log_action(f"å°†{col}åˆ—æ ‡å‡†åŒ–ä¸ºå¸ƒå°”ç±»å‹")
                    
                elif dtype == 'category':
                    self.df[col] = self.df[col].astype('category')
                    self.log_action(f"å°†{col}åˆ—è½¬æ¢ä¸ºåˆ†ç±»ç±»å‹")
                    
                elif dtype == 'text':
                    # æ¸…ç†æ–‡æœ¬æ•°æ®
                    self.df[col] = self.df[col].astype(str).str.strip()
                    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
                    self.df[col] = self.df[col].str.replace(r'\s+', ' ', regex=True)
                    self.log_action(f"æ¸…ç†{col}åˆ—çš„æ–‡æœ¬æ•°æ®")
                    
            except Exception as e:
                print(f"    è­¦å‘Šï¼š{col}åˆ—ç±»å‹è½¬æ¢å¤±è´¥ï¼š{e}")
                
    def clean_text_data(self):
        """æ¸…ç†æ–‡æœ¬æ•°æ®"""
        print("\n=== æ¸…ç†æ–‡æœ¬æ•°æ® ===")
        
        text_columns = self.df.select_dtypes(include=['object']).columns
        
        for col in text_columns:
            original_values = self.df[col].copy()
            
            # ç»Ÿä¸€å¤„ç†å­—ç¬¦ä¸²
            self.df[col] = self.df[col].astype(str)
            
            # ç§»é™¤é¦–å°¾ç©ºæ ¼
            self.df[col] = self.df[col].str.strip()
            
            # ç»Ÿä¸€å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
            self.df[col] = self.df[col].str.replace(r'\s+', ' ', regex=True)
            
            # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™å¸¸è§æ ‡ç‚¹ï¼‰
            self.df[col] = self.df[col].str.replace(r'[^\w\s\-\.\(\)â™€â™‚Â°â€²â€³]', '', regex=True)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹åŠ¨
            if not self.df[col].equals(original_values):
                self.log_action(f"æ¸…ç†äº†{col}åˆ—çš„æ–‡æœ¬æ ¼å¼")
                
    def validate_data_consistency(self):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        print("\n=== éªŒè¯æ•°æ®ä¸€è‡´æ€§ ===")
        
        # æ£€æŸ¥æ•°å€¼åˆ—çš„åˆç†æ€§
        numeric_columns = self.df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            if col.lower() in ['total', 'æ€»è®¡', 'sum']:
                # å¦‚æœæ˜¯æ±‡æ€»åˆ—ï¼Œæ£€æŸ¥æ˜¯å¦ç­‰äºå…¶ä»–åˆ—çš„å’Œ
                other_numeric_cols = [c for c in numeric_columns if c != col and 
                                    not any(keyword in c.lower() for keyword in ['total', 'sum', 'æ€»è®¡', 'id', '#'])]
                
                if len(other_numeric_cols) > 1:
                    calculated_total = self.df[other_numeric_cols].sum(axis=1)
                    current_total = self.df[col]
                    
                    # æ£€æŸ¥å·®å¼‚
                    diff = abs(calculated_total - current_total)
                    mismatched = diff > 1  # å®¹å¿1çš„è¯¯å·®
                    
                    if mismatched.sum() > 0:
                        print(f"  å‘ç°{mismatched.sum()}è¡Œ{col}åˆ—æ•°å€¼ä¸ä¸€è‡´")
                        # å¯ä»¥é€‰æ‹©ä¿®æ­£
                        # self.df[col] = calculated_total
                        # self.log_action(f"ä¿®æ­£äº†{col}åˆ—çš„è®¡ç®—å€¼")
                        
        print("âœ“ æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ")
        
    def generate_summary_report(self):
        """ç”Ÿæˆæ¸…æ´—æ‘˜è¦æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("           æ•°æ®æ¸…æ´—æ‘˜è¦æŠ¥å‘Š")
        print("="*60)
        
        print(f" è¾“å…¥æ–‡ä»¶ï¼š{self.input_file}")
        print(f" è¾“å‡ºæ–‡ä»¶ï¼š{self.output_file}")
        print(f" åŸå§‹æ•°æ®ï¼š{self.original_df.shape[0]}è¡Œ Ã— {self.original_df.shape[1]}åˆ—")
        print(f" æ¸…æ´—åï¼š{self.df.shape[0]}è¡Œ Ã— {self.df.shape[1]}åˆ—")
        
        # æ•°æ®å˜åŒ–ç»Ÿè®¡
        rows_changed = self.original_df.shape[0] - self.df.shape[0]
        if rows_changed != 0:
            print(f" è¡Œæ•°å˜åŒ–ï¼š{rows_changed:+d}")
            
        print(f"\nğŸ”§ æ‰§è¡Œçš„æ¸…æ´—æ“ä½œï¼ˆ{len(self.cleaning_log)}é¡¹ï¼‰ï¼š")
        for i, action in enumerate(self.cleaning_log, 1):
            print(f"   {i}. {action}")
            
        # æ•°æ®è´¨é‡æŒ‡æ ‡
        print(f"\n æ¸…æ´—åæ•°æ®è´¨é‡ï¼š")
        print(f"   - ç¼ºå¤±å€¼ï¼š{self.df.isnull().sum().sum()}")
        print(f"   - é‡å¤è¡Œï¼š{self.df.duplicated().sum()}")
        print(f"   - æ•°æ®ç±»å‹ï¼š{len(self.df.dtypes.unique())}ç§")
        
        # å„åˆ—çš„æ•°æ®ç±»å‹åˆ†å¸ƒ
        type_counts = self.df.dtypes.value_counts()
        print(f"\n æ•°æ®ç±»å‹åˆ†å¸ƒï¼š")
        for dtype, count in type_counts.items():
            print(f"   - {dtype}: {count}åˆ—")
            
    def save_cleaned_data(self):
        """ä¿å­˜æ¸…æ´—åçš„æ•°æ®"""
        try:
            self.df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
            print(f"\n æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°ï¼š{self.output_file}")
            return True
        except Exception as e:
            print(f" ä¿å­˜æ•°æ®å¤±è´¥ï¼š{e}")
            return False
            
    def run_cleaning_pipeline(self):
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®æ¸…æ´—æµç¨‹"""
        print(" å¯åŠ¨é€šç”¨è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—æµç¨‹")
        print("="*60)
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_data():
            return False
            
        # 2. è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹
        self.auto_detect_column_types()
        
        # 3. å¤„ç†ç¼ºå¤±å€¼
        self.detect_and_fix_missing_values()
        
        # 4. å¤„ç†é‡å¤æ•°æ®
        self.detect_and_remove_duplicates()
        
        # 5. å¤„ç†å¼‚å¸¸å€¼
        self.detect_and_fix_outliers()
        
        # 6. æ ‡å‡†åŒ–æ•°æ®ç±»å‹
        self.standardize_data_types()
        
        # 7. æ¸…ç†æ–‡æœ¬æ•°æ®
        self.clean_text_data()
        
        # 8. éªŒè¯æ•°æ®ä¸€è‡´æ€§
        self.validate_data_consistency()
        
        # 9. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        if self.save_cleaned_data():
            # 10. ç”ŸæˆæŠ¥å‘Š
            self.generate_summary_report()
            return True
        
        return False

def main():
    """ä¸»å‡½æ•°"""
    input_file = "e:/bigdatapractice/lab2/Pokemon.csv"
    output_file = "e:/bigdatapractice/lab2/Pokemon_cleaned.csv"
    
    print(" é€šç”¨è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—å·¥å…·")
    print("é€‚ç”¨äºä»»ä½•CSVæ•°æ®é›†ï¼Œæ— éœ€äº‹å…ˆäº†è§£æ•°æ®å†…å®¹")
    print("="*60)
    
    # åˆ›å»ºæ¸…æ´—å™¨å®ä¾‹
    cleaner = UniversalDataCleaner(input_file, output_file)
    
    # æ‰§è¡Œæ¸…æ´—æµç¨‹
    if cleaner.run_cleaning_pipeline():
        print("\n æ•°æ®æ¸…æ´—å·²å®Œæˆï¼")
    else:
        print("\n try againï¼")

if __name__ == "__main__":
    main()